{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0c9259",
   "metadata": {},
   "source": [
    "# Connectionist Temporal Classification (CTC)\n",
    "\n",
    "In Automatic Speech Recognition (ASR), a fundamental challenge is the **lack of explicit alignment**\n",
    "between audio frames and text tokens.\n",
    "\n",
    "Speech signals are continuous and vary in length, while transcriptions are discrete and much shorter.\n",
    "CTC provides a principled solution to this alignment problem by allowing models to learn\n",
    "the mapping from audio frames to text **without requiring frame-level labels**.\n",
    "\n",
    "This notebook introduces the theory behind CTC and implements a minimal,\n",
    "fully transparent CTC training pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a3534",
   "metadata": {},
   "source": [
    "## Why Alignment Is Hard in ASR\n",
    "\n",
    "Speech is inherently variable:\n",
    "- phonemes have different durations\n",
    "- speakers speak at different speeds\n",
    "- silence can appear anywhere\n",
    "\n",
    "Traditional supervised learning assumes one label per input.\n",
    "ASR violates this assumption because:\n",
    "- the number of audio frames >> number of text tokens\n",
    "- alignment is unknown\n",
    "\n",
    "CTC addresses this by marginalizing over all possible valid alignments\n",
    "between input frames and output tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcc80a",
   "metadata": {},
   "source": [
    "## How CTC Works\n",
    "\n",
    "CTC defines a many-to-one mapping:\n",
    "- many frame-level label sequences\n",
    "- map to one final transcription\n",
    "\n",
    "During training:\n",
    "- the model outputs a probability distribution over tokens (including blank) at each frame\n",
    "- CTC sums probabilities over all valid alignments\n",
    "- the loss is the negative log-likelihood of the correct transcription\n",
    "\n",
    "This allows learning **without explicit alignment**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfed5e",
   "metadata": {},
   "source": [
    "## Why Use CTC?\n",
    "\n",
    "CTC is particularly suitable for:\n",
    "- monotonic alignments (speech → text)\n",
    "- streaming ASR\n",
    "- simpler model architectures\n",
    "\n",
    "Compared to sequence-to-sequence models:\n",
    "- no attention mechanism\n",
    "- faster inference\n",
    "- simpler decoding\n",
    "- less data-hungry\n",
    "\n",
    "For this project, CTC provides clarity, interpretability,\n",
    "and a strong foundation for understanding modern ASR systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf523a",
   "metadata": {},
   "source": [
    "## Dependencies and Setup\n",
    "\n",
    "We use PyTorch’s built-in `CTCLoss`, which efficiently implements\n",
    "the forward-backward algorithm required for CTC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a12932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0135a",
   "metadata": {},
   "source": [
    "## Minimal Acoustic Model for CTC\n",
    "\n",
    "To demonstrate CTC mechanics, we define a minimal neural network that:\n",
    "- accepts Log-Mel features\n",
    "- outputs per-frame token probabilities\n",
    "\n",
    "This model is intentionally simple to keep the focus on CTC behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00396d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCTCModel(nn.Module):\n",
    "    def __init__(self, input_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, time, features)\n",
    "        returns: (time, batch, vocab)\n",
    "        \"\"\"\n",
    "        x = self.linear(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb83234",
   "metadata": {},
   "source": [
    "## Defining the CTC Loss\n",
    "\n",
    "CTC loss requires:\n",
    "- log probabilities (time, batch, vocab)\n",
    "- target sequences (concatenated)\n",
    "- input lengths (frames per utterance)\n",
    "- target lengths (tokens per utterance)\n",
    "\n",
    "The blank token index must be specified explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss_fn = nn.CTCLoss(\n",
    "    blank=1,          # <blank> index from tokenizer\n",
    "    reduction=\"mean\",\n",
    "    zero_infinity=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da120d",
   "metadata": {},
   "source": [
    "## Toy Example: Verifying CTC Behavior\n",
    "\n",
    "We construct a synthetic example to verify that:\n",
    "- dimensions are correct\n",
    "- loss computation works\n",
    "- CTC accepts variable-length inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "time_steps = 100\n",
    "n_mels = 80\n",
    "vocab_size = 30\n",
    "\n",
    "# Fake acoustic features\n",
    "inputs = torch.randn(batch_size, time_steps, n_mels)\n",
    "\n",
    "# Fake targets\n",
    "targets = torch.tensor([2, 3, 4, 5, 6, 7], dtype=torch.long)\n",
    "target_lengths = torch.tensor([3, 3], dtype=torch.long)\n",
    "input_lengths = torch.tensor([time_steps, time_steps], dtype=torch.long)\n",
    "\n",
    "model = SimpleCTCModel(n_mels, vocab_size)\n",
    "log_probs = model(inputs)\n",
    "\n",
    "loss = ctc_loss_fn(\n",
    "    log_probs,\n",
    "    targets,\n",
    "    input_lengths,\n",
    "    target_lengths\n",
    ")\n",
    "\n",
    "print(\"CTC loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64168a78",
   "metadata": {},
   "source": [
    "## Decoding with CTC \n",
    "\n",
    "During inference, the simplest decoding strategy is **greedy decoding**:\n",
    "- take argmax token at each frame\n",
    "- collapse repeats\n",
    "- remove blanks\n",
    "\n",
    "More advanced approaches (beam search, language models)\n",
    "can be layered on top, but greedy decoding is sufficient\n",
    "for validating the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fe8ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this step, we:\n",
    "- introduced the alignment problem in ASR,\n",
    "- explained the intuition and theory behind CTC,\n",
    "- implemented a minimal CTC-compatible model,\n",
    "- verified correct loss computation with synthetic data.\n",
    "\n",
    "With acoustic features, tokenized text, and CTC loss in place,\n",
    "the ASR pipeline is now complete at a conceptual level.\n",
    "\n",
    "The next step focuses on **end-to-end training and decoding**\n",
    "using real audio features and real transcriptions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
